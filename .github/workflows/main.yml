name: Crawl Hot Search

on:
  schedule:
    - cron: '7,37 * * * *'
  workflow_dispatch:

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        platform: [baidu, weibo, douyin, bilibili, zhihu, toutiao, tieba, quark, xiaohongshu]
      fail-fast: false
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      - name: Run crawler
        id: crawl
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          {
            echo 'output<<EOF'
            python main.py ${{ matrix.platform }} 2>&1 | tee /dev/stderr
            echo EOF
          } >> $GITHUB_OUTPUT
      
      - name: Add Job Summary
        if: always()
        run: |
          echo "## ${{ matrix.platform }} 爬取结果" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          echo "${{ steps.crawl.outputs.output }}" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
